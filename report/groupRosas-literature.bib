
@Article{kallet04methods,
  author = 	 {Richard H Kallet},
  title = 	 {How to Write the Methods Section of a Research Paper},
  journal = 	 {Respiratory Care},
  year = 	 2004,
  volume =	 49,
  number =	 10,
  pages =	 {1229--1232}
}

@Unpublished{anderson04,
  author = 		 {Greg Anderson},
  title = 		 {How to Write a Paper in Scientific Journal Style and Format},
  year = 	 2004,
  organization = {Bates College},
  note = {http://abacus.bates.edu/~ganderso/biology/resources/writing/HTWtoc.html}
}

@Unpublished{jones08,
  author = 		 {Simon Peyton Jones},
  title = 		 {How to write a great research paper},
  note = 		 {Microsoft Research Cambridge},
  year = 	 2008}


@article{Mnih2012,
abstract = {When training a system to label images, the amount of labeled training data tends to be a limiting factor. We consider the task of learning to label aerial images from existing maps. These provide abundant labels, but the labels are often incomplete and sometimes poorly registered. We propose two robust loss functions for dealing with these kinds of label noise and use the loss functions to train a deep neural network on two challenging aerial image datasets. The robust loss functions lead to big improvements in performance and our best system substantially outperforms the best published results on the task we consider. Copyright 2012 by the author(s)/owner(s).},
author = {Mnih, Volodymyr and Hinton, Geoffrey},
file = {:C\:/Users/Lucas/Google Drive/Studium/CIL/Project/Papers/Paper_VMinsh_NoisyMaps.pdf:pdf},
isbn = {9781450312851},
journal = {Proceedings of the 29th International Conference on Machine Learning, ICML 2012},
pages = {567--574},
title = {{Learning to label aerial images from noisy data}},
volume = {1},
year = {2012}
}
@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
eprint = {1706.03762},
file = {:C\:/Users/Lucas/Google Drive/Studium/CIL/Project/Papers/Paper_Vaswani_AttentionIsAllYouNeed_.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {5999--6009},
title = {{Attention is all you need}},
volume = {2017-December},
year = {2017}
}
@article{Petit2021,
abstract = {Medical image segmentation remains particularly challenging for complex and low-contrast anatomical structures. In this paper, we introduce the U-Transformer network, which combines a U-shaped architecture for image segmentation with self- and cross-attention from Transformers. U-Transformer overcomes the inability of U-Nets to model long-range contextual interactions and spatial dependencies, which are arguably crucial for accurate segmentation in challenging contexts. To this end, attention mechanisms are incorporated at two main levels: a self-attention module leverages global interactions between encoder features, while cross-attention in the skip connections allows a fine spatial recovery in the U-Net decoder by filtering out non-semantic features. Experiments on two abdominal CT-image datasets show the large performance gain brought out by U-Transformer compared to U-Net and local Attention U-Nets. We also highlight the importance of using both self- and cross-attention, and the nice interpretability features brought out by U-Transformer.},
archivePrefix = {arXiv},
arxivId = {2103.06104},
author = {Petit, Olivier and Thome, Nicolas and Rambour, Cl{\'{e}}ment and Soler, Luc},
eprint = {2103.06104},
file = {:C\:/Users/Lucas/Google Drive/Studium/CIL/Project/Papers/Paper_Petit_UNetTransformer_2021.pdf:pdf},
keywords = {cross-attention,global interactions,medical image segmentation,self-attention,spatial layout,transformers},
pages = {1--10},
title = {{U-Net Transformer: Self and Cross Attention for Medical Image Segmentation}},
url = {http://arxiv.org/abs/2103.06104},
year = {2021}
}
@article{Demir2018,
abstract = {We present the DeepGlobe 2018 Satellite Image Understanding Challenge, which includes three public competitions for segmentation, detection, and classification tasks on satellite images (Figure 1). Similar to other challenges in computer vision domain such as DAVIS[21] and COCO[33], DeepGlobe proposes three datasets and corresponding evaluation methodologies, coherently bundled in three competitions with a dedicated workshop co-located with CVPR 2018. We observed that satellite imagery is a rich and structured source of information, yet it is less investigated than everyday images by computer vision researchers. However, bridging modern computer vision with remote sensing data analysis could have critical impact to the way we understand our environment and lead to major breakthroughs in global urban planning or climate change research. Keeping such bridging objective in mind, DeepGlobe aims to bring together researchers from different domains to raise awareness of remote sensing in the computer vision community and vice-versa. We aim to improve and evaluate state-of-the-art satellite image understanding approaches, which can hopefully serve as reference benchmarks for future research in the same topic. In this paper, we analyze characteristics of each dataset, define the evaluation criteria of the competitions, and provide baselines for each task.},
archivePrefix = {arXiv},
arxivId = {1805.06561},
author = {Demir, Ilke and Koperski, Krzysztof and Lindenbaum, David and Pang, Guan and Huang, Jing and Basu, Saikat and Hughes, Forest and Tuia, Devis and Raska, Ramesh},
doi = {10.1109/CVPRW.2018.00031},
eprint = {1805.06561},
file = {:C\:/Users/Lucas/Google Drive/Studium/CIL/Project/Papers/Paper_Demir_DeepGlobe18.pdf:pdf},
isbn = {9781538661000},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
number = {May},
pages = {172--181},
title = {{DeepGlobe 2018: A challenge to parse the earth through satellite images}},
volume = {2018-June},
year = {2018}
}
@article{Chen2021,
abstract = {Medical image segmentation is an essential prerequisite for developing healthcare systems, especially for disease diagnosis and treatment planning. On various medical image segmentation tasks, the u-shaped architecture, also known as U-Net, has become the de-facto standard and achieved tremendous success. However, due to the intrinsic locality of convolution operations, U-Net generally demonstrates limitations in explicitly modeling long-range dependency. Transformers, designed for sequence-to-sequence prediction, have emerged as alternative architectures with innate global self-attention mechanisms, but can result in limited localization abilities due to insufficient low-level details. In this paper, we propose TransUNet, which merits both Transformers and U-Net, as a strong alternative for medical image segmentation. On one hand, the Transformer encodes tokenized image patches from a convolution neural network (CNN) feature map as the input sequence for extracting global contexts. On the other hand, the decoder upsamples the encoded features which are then combined with the high-resolution CNN feature maps to enable precise localization. We argue that Transformers can serve as strong encoders for medical image segmentation tasks, with the combination of U-Net to enhance finer details by recovering localized spatial information. TransUNet achieves superior performances to various competing methods on different medical applications including multi-organ segmentation and cardiac segmentation. Code and models are available at https://github.com/Beckschen/TransUNet.},
archivePrefix = {arXiv},
arxivId = {2102.04306},
author = {Chen, Jieneng and Lu, Yongyi and Yu, Qihang and Luo, Xiangde and Adeli, Ehsan and Wang, Yan and Lu, Le and Yuille, Alan L. and Zhou, Yuyin},
eprint = {2102.04306},
file = {:C\:/Users/Lucas/Google Drive/Studium/CIL/Project/Papers/Paper_Chen_TransUNet_2021.pdf:pdf},
pages = {1--13},
title = {{TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation}},
url = {http://arxiv.org/abs/2102.04306},
year = {2021}
}
@article{Azimi2019,
abstract = {Understanding the complex urban infrastructure with centimeter-level accuracy is essential for many applications from autonomous driving to mapping, infrastructure monitoring, and urban management. Aerial images provide valuable information over a large area instantaneously; nevertheless, no current dataset captures the complexity of aerial scenes at the level of granularity required by real-world applications. To address this, we introduce SkyScapes, an aerial image dataset with highly-accurate, fine-grained annotations for pixel-level semantic labeling. SkyScapes provides annotations for 31 semantic categories ranging from large structures, such as buildings, roads and vegetation, to fine details, such as 12 (sub-)categories of lane markings. We have defined two main tasks on this dataset: Dense semantic segmentation and multi-class lane-marking prediction. We carry out extensive experiments to evaluate state-of-the-art segmentation methods on SkyScapes. Existing methods struggle to deal with the wide range of classes, object sizes, scales, and fine details present. We therefore propose a novel multi-task model, which incorporates semantic edge detection and is better tuned for feature extraction from a wide range of scales. This model achieves notable improvements over the baselines in region outlines and level of detail on both tasks.},
archivePrefix = {arXiv},
arxivId = {2007.06102},
author = {Azimi, Seyed Majid and Henry, Corentin and Sommer, Lars and Schumann, Arne and Vig, Eleonora},
doi = {10.1109/ICCV.2019.00749},
eprint = {2007.06102},
file = {:C\:/Users/Lucas/Google Drive/Studium/CIL/Project/Papers/Paper_Azimi_SkyScapes_2019.pdf:pdf},
isbn = {9781728148038},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {7392--7402},
title = {{Skyscapes - fine-grained semantic understanding of aerial scenes}},
volume = {2019-October},
year = {2019}
}

@INPROCEEDINGS{7780459,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}}

@article{DBLP:journals/corr/abs-2007-06102,
  author    = {Seyed Majid Azimi and
               Corentin Henry and
               Lars Sommer and
               Arne Schumann and
               Eleonora Vig},
  title     = {SkyScapes - Fine-Grained Semantic Understanding of Aerial Scenes},
  journal   = {CoRR},
  volume    = {abs/2007.06102},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.06102},
  archivePrefix = {arXiv},
  eprint    = {2007.06102},
  timestamp = {Tue, 21 Jul 2020 12:53:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-06102.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/RonnebergerFB15,
  author    = {Olaf Ronneberger and
               Philipp Fischer and
               Thomas Brox},
  title     = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  journal   = {CoRR},
  volume    = {abs/1505.04597},
  year      = {2015},
  url       = {http://arxiv.org/abs/1505.04597},
  archivePrefix = {arXiv},
  eprint    = {1505.04597},
  timestamp = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RonnebergerFB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{PAL19931277,
title = {A review on image segmentation techniques},
journal = {Pattern Recognition},
volume = {26},
number = {9},
pages = {1277-1294},
year = {1993},
issn = {0031-3203},
doi = {https://doi.org/10.1016/0031-3203(93)90135-J},
url = {https://www.sciencedirect.com/science/article/pii/003132039390135J},
author = {Nikhil R Pal and Sankar K Pal},
keywords = {Image segmentation, Fuzzy sets, Thresholding, Edge detection, Clustering, Relaxation, Markov Random Field},
abstract = {Many image segmentation techniques are available in the literature. Some of these techniques use only the gray level histogram, some use spatial details while others use fuzzy set theoretic approaches. Most of these techniques are not suitable for noisy environments. Some works have been done using the Markov Random Field (MRF) model which is robust to noise, but is computationally involved. Neural network architectures which help to get the output in real time because of their parallel processing ability, have also been used for segmentation and they work fine even when the noise level is very high. The literature on color image segmentation is not that rich as it is for gray tone images. This paper critically reviews and summarizes some of these techniques. Attempts have been made to cover both fuzzy and non-fuzzy techniques including color image segmentation and neural network based approaches. Adequate attention is paid to segmentation of range images and magnetic resonance images. It also addresses the issue of quantitative evaluation of segmentation results.}
}

@article{DBLP:journals/corr/ChenPSA17,
  author    = {Liang{-}Chieh Chen and
               George Papandreou and
               Florian Schroff and
               Hartwig Adam},
  title     = {Rethinking Atrous Convolution for Semantic Image Segmentation},
  journal   = {CoRR},
  volume    = {abs/1706.05587},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.05587},
  archivePrefix = {arXiv},
  eprint    = {1706.05587},
  timestamp = {Mon, 13 Aug 2018 16:48:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChenPSA17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-1805-06561,
  author    = {Ilke Demir and
               Krzysztof Koperski and
               David Lindenbaum and
               Guan Pang and
               Jing Huang and
               Saikat Basu and
               Forest Hughes and
               Devis Tuia and
               Ramesh Raskar},
  title     = {DeepGlobe 2018: {A} Challenge to Parse the Earth through Satellite
               Images},
  journal   = {CoRR},
  volume    = {abs/1805.06561},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.06561},
  archivePrefix = {arXiv},
  eprint    = {1805.06561},
  timestamp = {Thu, 20 Feb 2020 18:58:02 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-06561.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.5555/3042573.3042603,
author = {Mnih, Volodymyr and Hinton, Geoffrey},
title = {Learning to Label Aerial Images from Noisy Data},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {When training a system to label images, the amount of labeled training data tends
to be a limiting factor. We consider the task of learning to label aerial images from
existing maps. These provide abundant labels, but the labels are often incomplete
and sometimes poorly registered. We propose two robust loss functions for dealing
with these kinds of label noise and use the loss functions to train a deep neural
network on two challenging aerial image datasets. The robust loss functions lead to
big improvements in performance and our best system substantially outperforms the
best published results on the task we consider.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {203–210},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@article{Montoya2015,
author = {Montoya-Zegarra, Javier A. and Wegner, Jermaine and Ladický, L. and Schindler, Konrad},
year = {2015},
month = {03},
pages = {127-133},
title = {SEMANTIC SEGMENTATION OF AERIAL IMAGES IN URBAN AREAS WITH CLASS-SPECIFIC HIGHER-ORDER CLIQUES},
volume = {II-3/W4},
journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
doi = {10.5194/isprsannals-II-3-W4-127-2015}
}

@article{Marmanis2016,
author = {Marmanis, Dimitris and Wegner, Jermaine and Galliani, Silvano and Schindler, Konrad and Datcu, Mihai and Stilla, Uwe},
year = {2016},
month = {06},
pages = {473-480},
title = {SEMANTIC SEGMENTATION OF AERIAL IMAGES WITH AN ENSEMBLE OF CNNS},
volume = {III-3},
journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
doi = {10.5194/isprs-annals-III-3-473-2016}
}

@article{GUPTA202122,
title = {Deep learning-based aerial image segmentation with open data for disaster impact assessment},
journal = {Neurocomputing},
volume = {439},
pages = {22-33},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.02.139},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221001429},
author = {Ananya Gupta and Simon Watson and Hujun Yin},
keywords = {Disaster response, Aerial images, Semantic segmentation, Convolutional neural networks, Graph theory},
abstract = {Satellite images are an extremely valuable resource in the aftermath of natural disasters such as hurricanes and tsunamis where they can be used for risk assessment and disaster management. In order to provide timely and actionable information for disaster response, a framework utilising segmentation neural networks is proposed to identify impacted areas and accessible roads in post-disaster scenarios. The effectiveness of pretraining with ImageNet -for the task of aerial image segmentation has been analysed and performances of popular segmentation models compared. Experimental results show that pretraining on ImageNet usually improves the segmentation performance for a number of models. Open data available from OpenStreetMap (OSM) is used for training, forgoing the need for time-consuming manual annotation. The method also makes use of graph theory to update road network data available from OSM and to detect the changes caused by a natural disaster. Extensive experiments on data from the 2018 tsunami that struck Palu, Indonesia show the effectiveness of the proposed framework. ENetSeparable, with 30% fewer parameters compared to ENet, achieved comparable segmentation results to that of the state-of-the-art networks.}
}

@article{Superroad2021,
title = {Superroad: Road segmentation through multi-objective ensemble and
geometric-aware post-processing},
year = {2019},
url = {https://azuxmioy.github.io/papers/SuperRoad.pdf},
author = {Davin Choo, Hsuan-I Ho, Juan-Ting Lin and Vaclav Rozhon},
abstract = {Road segmentation is a popular computer com-
puter vision task. In this project, we modified a state-of-the-art
convolutional neural network and developed a geometric-aware
post-processing pipeline. As neural networks are data-hungry,
we augment the lack of training data from the CIL dataset
with external datasets. To further boost performance, we used
an ensemble of models. With these techniques, we were able
to rank 2ndin the both the public and private leaderboards
of the 2019 CIL road segmentation Kaggle contest.}
}

@article{liaw2018tune,
    title={Tune: A Research Platform for Distributed Model Selection and Training},
    author={Liaw, Richard and Liang, Eric and Nishihara, Robert
            and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion},
    journal={arXiv preprint arXiv:1807.05118},
    year={2018}
}
@misc{oktay2018attention,
      title={Attention U-Net: Learning Where to Look for the Pancreas}, 
      author={Ozan Oktay and Jo Schlemper and Loic Le Folgoc and Matthew Lee and Mattias Heinrich and Kazunari Misawa and Kensaku Mori and Steven McDonagh and Nils Y Hammerla and Bernhard Kainz and Ben Glocker and Daniel Rueckert},
      year={2018},
      eprint={1804.03999},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{alom2018recurrent,
      title={Recurrent Residual Convolutional Neural Network based on U-Net (R2U-Net) for Medical Image Segmentation}, 
      author={Md Zahangir Alom and Mahmudul Hasan and Chris Yakopcic and Tarek M. Taha and Vijayan K. Asari},
      year={2018},
      eprint={1802.06955},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{ronneberger2015unet,
      title={U-Net: Convolutional Networks for Biomedical Image Segmentation}, 
      author={Olaf Ronneberger and Philipp Fischer and Thomas Brox},
      year={2015},
      eprint={1505.04597},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article{DBLP:journals/corr/abs-1906-07160,
  author    = {Malav Bateriwala and
               Pierrick Bourgeat},
  title     = {Enforcing temporal consistency in Deep Learning segmentation of brain
               {MR} images},
  journal   = {CoRR},
  volume    = {abs/1906.07160},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.07160},
  archivePrefix = {arXiv},
  eprint    = {1906.07160},
  timestamp = {Mon, 24 Jun 2019 17:28:45 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1906-07160},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{dosovitskiy2020vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal={ICLR},
  year={2021}
}
